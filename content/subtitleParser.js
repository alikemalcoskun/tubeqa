/**
 * Subtitle Parser for YouTube AI Q&A Assistant
 * Fetches and parses caption track data from YouTube's internal APIs:
 * First, it finds the best available caption track.
 * Then, it triggers caption requests to generate signed timedtext requests.
 * Then, it fetches the full transcript for the video.
 * Then, it extracts the transcript text for the current time range.
 * Then, it caches the transcript text for the current time range.
 * Then, it returns the transcript text for the current time range.
 */

class SubtitleParser {
  constructor() {
    this.captionTracks = [];
    this.currentTrack = null;
    this.lastFetchTime = 0;
    this.cache = new Map(); // Cache for time-specific chunks
    this.fullTranscriptCache = new Map(); // Cache for full video transcripts
  }

  /**
   * Initialize the subtitle parser by finding available caption tracks
   * @returns {Promise<boolean>} Success status
   */
  async initialize() {
    try {
      // Wait for YouTube player to be ready
      await this.waitForPlayer();

      // Extract caption tracks from YouTube's player response
      const tracks = this.extractCaptionTracks();
      this.captionTracks = tracks;

      if (tracks.length === 0) {
        console.warn('No caption tracks found for this video');
        return false;
      }

      // Select the best available track (prefer English, then any auto-generated)
      this.currentTrack = this.selectBestTrack(tracks);

      return true;
    } catch (error) {
      console.error('Failed to initialize subtitle parser:', error);
      return false;
    }
  }

  /**
   * Wait for the YouTube player to be fully loaded
   * @returns {Promise<void>}
   */
  async waitForPlayer() {
    return new Promise((resolve, reject) => {
      const maxAttempts = 50; // 5 seconds max wait
      let attempts = 0;

      const checkPlayer = () => {
        attempts++;

        // Check for various YouTube player ready states
        const player = document.querySelector('#movie_player') ||
                      document.querySelector('.html5-video-player') ||
                      window.ytInitialPlayerResponse;

        if (player || attempts >= maxAttempts) {
          resolve();
        } else {
          setTimeout(checkPlayer, 100);
        }
      };

      checkPlayer();
    });
  }

  /**
   * Extract caption tracks from YouTube's internal data structures
   * @returns {Array} Array of available caption tracks
   */
  extractCaptionTracks() {
    const tracks = [];
  
    try {
      // window.ytInitialPlayerResponse
      if (window.ytInitialPlayerResponse?.captions?.playerCaptionsTracklistRenderer?.captionTracks) {
        const ytTracks = window.ytInitialPlayerResponse.captions.playerCaptionsTracklistRenderer.captionTracks;
        tracks.push(...this.parseYouTubeTracks(ytTracks));
      } else {
        // Fallback: Fetch from HTML
        const script = [...document.scripts].find(s => s.textContent.includes('ytInitialPlayerResponse'));
        if (script) {
          const jsonMatch = script.textContent.match(/ytInitialPlayerResponse\s*=\s*(\{.*?\});/s);
          if (jsonMatch && jsonMatch[1]) {
            const data = JSON.parse(jsonMatch[1]);
            const ytTracks = data?.captions?.playerCaptionsTracklistRenderer?.captionTracks;
            if (ytTracks) {
              tracks.push(...this.parseYouTubeTracks(ytTracks));
            }
          }
        }
      }
  
      // Backward compatibility: ytplayer.config
      if (tracks.length === 0 && window.ytplayer?.config?.args?.player_response) {
        try {
          const response = JSON.parse(window.ytplayer.config.args.player_response);
          if (response?.captions?.playerCaptionsTracklistRenderer?.captionTracks) {
            const ytTracks = response.captions.playerCaptionsTracklistRenderer.captionTracks;
            tracks.push(...this.parseYouTubeTracks(ytTracks));
          }
        } catch {}
      }
  
      // Remove duplicates
      const uniqueTracks = tracks.filter(
        (track, index, self) => index === self.findIndex(t => t.url === track.url)
      );

      console.log('Unique tracks:', uniqueTracks);
  
      return uniqueTracks;
    } catch (error) {
      console.error('Error extracting caption tracks:', error);
      return [];
    }
  }

  /**
   * Parse YouTube's internal track format into our format
   * @param {Array} ytTracks - YouTube's caption tracks
   * @returns {Array} Parsed tracks
   */
  parseYouTubeTracks(ytTracks) {
    return ytTracks.map(track => ({
      language: track.languageCode,
      name: track.name?.simpleText || track.name?.runs?.[0]?.text || track.languageCode,
      url: track.baseUrl,
      isAutoGenerated: track.kind === 'asr',
      isDefault: track.isDefault || false
    }));
  }

  /**
   * Select the best available caption track
   * @param {Array} tracks - Available tracks
   * @returns {Object|null} Best track or null
   */
  selectBestTrack(tracks) {
    if (tracks.length === 0) return null;

    // Priority: English > Auto generated > Default > Any available track
    const englishTrack = tracks.find(track => track.language.startsWith('en'));
    if (englishTrack) {
      console.log('Found English track:', englishTrack);
      return englishTrack;
    }

    // Auto generated track
    const autoGeneratedTrack = tracks.find(track => track.name === 'English (auto-generated)');
    if (autoGeneratedTrack) {
      console.log('Found Auto generated track:', autoGeneratedTrack);
      return autoGeneratedTrack;
    }

    // Default track
    const defaultTrack = tracks.find(track => track.isDefault);
    if (defaultTrack) {
      console.log('Found Default track:', defaultTrack);
      return defaultTrack;
    }

    // Return first available track as fallback
    console.log('No track found, returning first available track:', tracks[0]);
    return tracks[0];
  }

  /**
   * Get transcript text for the current time range
   * @param {number} duration - How many seconds of transcript to fetch (default: 60)
   * @param {number} bufferAhead - How many seconds ahead to buffer (default: 0)
   * @returns {Promise<string>} Transcript text
   */
  async getTranscriptChunk(duration = 60, bufferAhead = 30) {
    if (!this.currentTrack) {
      console.warn('No caption track available');
      return '';
    }

    try {
      const currentTime = this.getCurrentVideoTime();
      // Calculate time range with buffering: some past + buffer ahead
      const startTime = currentTime;
      const endTime = currentTime + bufferAhead;
      const videoId = this.getCurrentVideoId();

      // Check chunk cache first
      const chunkCacheKey = `${videoId}_${startTime}_${endTime}`;
      if (this.cache.has(chunkCacheKey)) {
        return this.cache.get(chunkCacheKey);
      }

      // Check if we have full transcript cached for this video
      let fullTranscriptData = this.fullTranscriptCache.get(videoId);

      if (!fullTranscriptData) {
        console.log('Fetching full transcript for video:', videoId);

        // Step 1: Trigger caption changes to generate signed timedtext requests
        await this.triggerCaptionRequests();
        // Wait for 3 seconds
        await new Promise(resolve => setTimeout(resolve, 3000));

        // Step 2: Get the signed URL
        const signedUrl = await this.getSignedTimedTextUrl();
        if (!signedUrl) {
          console.warn('No signed caption URL available');
          return '';
        }

        console.log('Signed timedtext URL found, fetching full transcript');

        // Step 3: Fetch FULL transcript (0 to Infinity)
        fullTranscriptData = await this.fetchCaptionData(signedUrl, 0, Infinity);

        // Step 4: Cache the full transcript
        this.fullTranscriptCache.set(videoId, fullTranscriptData);
        console.log('Cached full transcript for video:', videoId);
      }

      // Step 5: Extract and slice the relevant time range
      const slicedData = {
        events: fullTranscriptData.events.filter(event =>
          event.start >= startTime && event.start <= endTime
        )
      };

      const transcript = this.extractTranscriptText(slicedData);
      console.log(`Extracted transcript chunk: ${slicedData.events.length} events, ${transcript.length} chars`);

      // Cache the chunk result
      this.cache.set(chunkCacheKey, transcript);

      return transcript;
    } catch (error) {
      console.error('Failed to get transcript chunk:', error);
      return '';
    }
  }

  /**
   * Get the current signed timedtext URL from performance entries or player metadata
   * @returns {Promise<string|null>} Signed caption URL or null if not found
   */
  async getSignedTimedTextUrl() {
    try {
      // Step 1: Check for recent signed URL in global variable or performance entries
      let signedUrl = this.getSignedUrlFromPerformance();

      if (signedUrl) {
        console.log('Found signed URL from player metadata');
        return signedUrl;
      }

      console.warn('No signed caption URL found');
      return null;
    } catch (error) {
      console.error('Error getting signed timedtext URL:', error);
      return null;
    }
  }

  /**
   * Get signed URL from performance entries
   * @returns {string|null} Signed URL or null
   */
  getSignedUrlFromPerformance() {
    try {
      if (!performance || !performance.getEntriesByType) {
        return null;
      }

      const resources = performance.getEntriesByType('resource');
      const timedtextRequests = resources
        .filter(entry => entry.name && entry.name.includes('/api/timedtext'))
        .sort((a, b) => b.startTime - a.startTime); // Most recent first

      if (timedtextRequests.length > 0) {
        return timedtextRequests[0].name;
      }

      return null;
    } catch (error) {
      console.warn('Error checking performance entries:', error);
      return null;
    }
  }

  /**
   * Get the current caption state
   * @returns {Object} Current caption state
   */
  getCurrentCaptionState() {
    try {
      // Check if captions are enabled via player API
      const player = document.querySelector('#movie_player') ||
                    document.querySelector('.html5-video-player');

      if (player && player.getOption) {
        try {
          const captionsEnabled = player.getOption('captions', 'track') !== null;
          const track = player.getOption('captions', 'track');
          return {
            enabled: captionsEnabled,
            language: track ? track.languageCode : null,
            track: track
          };
        } catch (e) {
          // Player API not available
        }
      }

      // Fallback: Check DOM elements
      const ccButton = document.querySelector('.ytp-subtitles-button.ytp-button-active') ||
                      document.querySelector('[aria-label*="subtitles"][aria-pressed="true"]') ||
                      document.querySelector('[aria-label*="captions"][aria-pressed="true"]');

      return {
        enabled: !!ccButton,
        language: null, // Can't determine from DOM
        track: null
      };
    } catch (error) {
      console.warn('Error getting current caption state:', error);
      return { enabled: false, language: null, track: null };
    }
  }

  /**
   * Check if English captions are available
   * @returns {boolean} True if English captions are available
   */
  areEnglishCaptionsAvailable() {
    try {
      // Get available tracks
      const tracks = this.extractCaptionTracks();
      return tracks.some(track => track.language.startsWith('en'));
    } catch (error) {
      console.warn('Error checking English captions availability:', error);
      return false;
    }
  }

  /**
   * Trigger caption changes to generate signed timedtext requests
   * This enables captions (preferably English) to create the signed API calls we need
   * @returns {Promise<void>}
   */
  async triggerCaptionRequests() {
    try {
      // Check if English captions are already enabled - if so, we're good
      const currentState = this.getCurrentCaptionState();
      if (currentState.enabled && currentState.language && currentState.language.startsWith('en')) {
        console.log('English captions already enabled');
        return;
      }

      // Check if English captions are available
      if (!this.areEnglishCaptionsAvailable()) {
        console.warn('English captions not available - will use whatever is available');
        // Still try to trigger any captions to generate a request
      }

      // Try to enable English captions via player API (fastest method)
      const player = document.querySelector('#movie_player') ||
                    document.querySelector('.html5-video-player');

      if (player && player.setOption) {
        try {
          // Find English track
          const tracks = this.extractCaptionTracks();
          const englishTrack = tracks.find(track => track.language.startsWith('en'));

          if (englishTrack) {
            player.setOption('captions', 'track', { languageCode: englishTrack.language });
            console.log('Triggered English captions via player API');
            return;
          }
        } catch (e) {
          console.warn('Failed to enable captions via player API:', e);
        }
      }

      // Fallback: Click the CC button to trigger caption loading
      const ccButton = document.querySelector('.ytp-subtitles-button') ||
                      document.querySelector('[aria-label*="subtitles"]') ||
                      document.querySelector('[aria-label*="captions"]');

      if (ccButton) {
        // Click to toggle captions (this will generate the signed request)
        ccButton.click();
        console.log('Triggered captions via CC button click');

        // Immediately click again to restore state (but request is already sent)
        setTimeout(() => {
          if (ccButton) ccButton.click();
        }, 100);
      }

    } catch (error) {
      console.warn('Error triggering English captions:', error);
    }
  }


  /**
   * Get the current video ID from URL
   * @returns {string} Video ID or empty string if not found
   */
  getCurrentVideoId() {
    try {
      const url = new URL(window.location.href);
      return url.searchParams.get('v') || '';
    } catch (error) {
      console.warn('Error getting video ID:', error);
      return '';
    }
  }

  /**
   * Get the current video playback time
   * @returns {number} Current time in seconds
   */
  getCurrentVideoTime() {
    const video = document.querySelector('video');
    return video ? video.currentTime : 0;
  }

  /**
   * Fetch and parse the caption data from signed timedtext URL
   * Once a signed URL is available, call: append fmt=json3 (for JSON) or fall back to fmt=srv3 (XML).
   * Parse into caption events: { "start": Number, "dur": Number, "text": String }
   * Filter by time range: include only events where (start + dur) ≥ startTime && start ≤ endTime.
   *
   * @param {string} url - Signed YouTube timedtext URL
   * @param {number} [startTime=0] - Start time in seconds
   * @param {number} [endTime=Infinity] - End time in seconds
   * @returns {Promise<{ events: Array<{ start: number, dur: number, text: string }> }>}
   */
  async fetchCaptionData(baseUrl, startTime = 0, endTime = Infinity) {
    try {
      const url = new URL(baseUrl);
      console.log('URL:', url.toString());
      url.searchParams.set("fmt", "json3");

      const response = await fetch(url.toString());
      const rawText = await response.text();
      console.log('Raw text:', rawText);
      let data;

      // Try JSON first
      try {
        const json = JSON.parse(rawText.replace(/^\uFEFF/, ""));
        console.log('JSON:', json);
        const events = (json.events || [])
          .map(ev => {
            const text = (ev.segs || [])
              .map(seg => seg.utf8)
              .join("")
              .trim();
            return {
              start: ev.tStartMs / 1000,
              dur: (ev.dDurationMs || 0) / 1000,
              text
            };
          })
          // Filter by startTime / endTime
          .filter(e => e.start + e.dur >= startTime && e.start <= endTime && e.text);
        data = { events };
      } catch {
        // Fallback to XML format (srv3)
        const xmlUrl = new URL(baseUrl);
        xmlUrl.searchParams.set("fmt", "srv3");
        const xmlResponse = await fetch(xmlUrl.toString());
        const xmlText = await xmlResponse.text();
        console.log('XML text:', xmlText);
        const parser = new DOMParser();
        const xmlDoc = parser.parseFromString(xmlText, "text/xml");
        console.log('XML doc:', xmlDoc);
        const events = Array.from(xmlDoc.querySelectorAll("text"))
          .map(node => ({
            start: parseFloat(node.getAttribute("start") || "0"),
            dur: parseFloat(node.getAttribute("dur") || "0"),
            text: node.textContent
          }))
          // Filter by time range
          .filter(e => e.start + e.dur >= startTime && e.start <= endTime && e.text);
        console.log('Events:', events);
        data = { events };
      }

      console.log(`Parsed ${data.events.length} caption events between ${startTime}s–${endTime === Infinity ? 'end' : endTime + 's'}`);
      console.log('Data:', data);
      return data;
    } catch (error) {
      console.error("Failed to fetch caption data:", error);
      return { events: [] };
    }
  }


  /**
   * Extract readable text from caption data
   * @param {Object} captionData - Caption data from YouTube
   * @returns {string} Concatenated transcript text
   */
  extractTranscriptText(captionData) {
    console.log('Caption data:', captionData);
    if (!captionData || !captionData.events) {
      console.log('No caption data or events');
      return '';
    }

    const transcript = captionData.events
      .filter(event => {
        // Check if event has direct text field (processed format)
        if (event.text && event.text.trim()) return true;
        // Or check if event has segs array (raw YouTube format)
        if (event.segs && event.segs.length > 0) return true;
        return false;
      })
      .map(event => {
        // If event has direct text field, use it
        if (event.text) {
          return event.text.trim();
        }
        // Otherwise, extract from segs array
        return event.segs
          .map(seg => seg.utf8 || '')
          .join('')
          .trim();
      })
      .filter(text => text.length > 0)
      .join(' ')
      .replace(/\s+/g, ' ') // Normalize whitespace
      .trim();
    console.log('Transcript:', transcript);
    return transcript;
  }

  /**
   * Force refresh of caption tracks (useful if video changes)
   */
  async refreshTracks() {
    this.captionTracks = [];
    this.currentTrack = null;
    this.cache.clear();
    this.fullTranscriptCache.clear();
    await this.initialize();
  }

  /**
   * Get cache statistics for debugging
   * @returns {Object} Cache statistics
   */
  getCacheStats() {
    return {
      chunkCache: {
        size: this.cache.size,
        keys: Array.from(this.cache.keys())
      },
      fullTranscriptCache: {
        size: this.fullTranscriptCache.size,
        keys: Array.from(this.fullTranscriptCache.keys())
      }
    };
  }
}

// Create global instance for convenience functions
window.__subtitleParser = new SubtitleParser();

// Global exports for browser automation
window.getSignedTimedTextUrl = async function() {
  return await window.__subtitleParser.getSignedTimedTextUrl();
};

window.fetchCaptionData = async function(url, startTime = 0, endTime = Infinity) {
  return await window.__subtitleParser.fetchCaptionData(url, startTime, endTime);
};

window.fetchCaptionsWhenCaptionsOn = async function(startTime = 0, endTime = Infinity) {
  // Get signed URL (this handles initialization internally)
  const signedUrl = await window.__subtitleParser.getSignedTimedTextUrl();
  if (!signedUrl) {
    throw new Error('No signed caption URL available. Make sure captions are enabled and a video is playing.');
  }

  // Fetch and return data
  return await window.__subtitleParser.fetchCaptionData(signedUrl, startTime, endTime);
};

window.getCaptionCacheStats = function() {
  return window.__subtitleParser.getCacheStats();
};

// Export for use in other modules
window.SubtitleParser = SubtitleParser;
